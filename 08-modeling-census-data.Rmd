# Modeling US Census data

```{r setup-ch8, include = FALSE}
knitr::opts_chunk$set(warning = FALSE, message = FALSE)
options(tigris_use_cache = TRUE)
minutes_to_downtown <- readr::read_rds("data/ch8_minutes_to_downtown.rds")
source("R/book-functions.R")
```

The previous chapter included a range of examples illustrating methods for analyzing and exploring spatial datasets. Census data can also be used to derive models for explaining patterns that occur across regions or within cities. These models draw from concepts introduced in prior chapters, but can also be used as part of explanatory frameworks or within broader analytic pipelines for statistical inference or machine learning. This chapter introduces a series of such frameworks. The first section looks at *demographic indices* such as segregation, diversity, and centralization, which are widely used across the social sciences to explain demographic patterns. The second section explores topics in statistical modeling, including methods for *spatial regression* that take into account the spatial autocorrelation inherent in most Census variables. The third and final section explores concepts such as *classification*, *clustering*, and *regionalization* which are common in both unsupervised and supervised machine learning. Examples will illustrate how to use Census data to generate neighborhood typologies, which are widely used for business and marketing applications, and how to generate spatially coherent sales territories from Census data with regionalization.

## Indices of segregation and diversity

A large body of research in the social sciences is concerned with neighborhood *segregation* and *diversity*. Segregation as addressed here generally refers to the measurement of the extent to which two or more groups live apart from each other; diversity as a companion metric measures neighborhood heterogeneity among groups. A wide range of indices have been developed by social scientists to measure segregation and diversity, and in many cases are inherently linked with spatial Census data which are often the best way to measure these concepts. Segregation and diversity indices are implemented in a variety of different R packages; the package recommended by this book is the segregation package [@elbers2021], which includes R functions for a variety of regional and local indices.

Much of the segregation and diversity literature focuses on race and ethnicity, which will be explored in the example below. The data setup code uses spatial methods covered in the previous three chapters to acquire Census tract-level data on population estimates for non-Hispanic white, non-Hispanic black, non-Hispanic Asian, and Hispanic populations in California, then filters those Census tracts for those that intersect the boundary of the Los Angeles urbanized area.

```{r get-race-data-ch8}
library(tidycensus)
library(tidyverse)
library(segregation) # remotes::install_github("elbersb/segregation")
library(tigris)
library(sf)

la_urban_area <- urban_areas(cb = TRUE, year = 2019) %>%
  filter(NAME10 == "Los Angeles--Long Beach--Anaheim, CA")

la_data <- get_acs(
  geography = "tract",
  variables = c(
    white = "B03002_003",
    black = "B03002_004",
    asian = "B03002_006",
    hispanic = "B03002_012"
  ), 
  state = "CA",
  geometry = TRUE,
  year = 2019
) %>%
  st_filter(la_urban_area) %>%
  st_drop_geometry()
```

The data structure appears as follows:

```{r}
style_data(la_data, n_rows = 8)
```

The data are in long (tidy) form, the default used by tidycensus; this data structure is ideal for computing indices in the segregation package.

### The dissimilarity index

The dissimilarity index is widely used to assess neighborhood segregation between two groups within a region. It is computed as follows:

$$
D = formula()
$$

The index ranges from a low of 0 to a high of 1, where 0 represents perfect integration between the two groups and 1 represents complete segregation. This index is implemented in the segregation package with the `dissimilarity()` function.

```{r dissimilarity}
la_data %>%
  filter(variable %in% c("white", "black")) %>%
  dissimilarity(
    group = "variable",
    unit = "GEOID",
    weight = "estimate"
  )
```

### Entropy indices

Multi-group entropy:

```{r multi-entropy}
la_data %>%
  mutual_total(
    group = "variable",
    unit = "GEOID",
    weight = "estimate"
  )
```

Local segregation indices:

```{r map-local-seg}

la_local_seg <- la_data %>%
  mutual_local(
    group = "variable",
    unit = "GEOID",
    weight = "estimate", 
    wide = TRUE
  )

glimpse(la_local_seg)
```

Map the results:

```{r}
ca_tracts <- tracts("CA", cb = TRUE)

ca_tracts %>%
  inner_join(la_local_seg, by = "GEOID") %>%
  ggplot(aes(fill = ls)) + 
  geom_sf(color = NA) + 
  coord_sf(crs = 26946) + 
  scale_fill_viridis_c(option = "inferno") + 
  theme_void() + 
  labs(fill = "Local\nsegregation index")
```

### Visualizing the diversity gradient

The *diversity gradient* is a concept that uses scatterplot smoothing to visualize how neighborhood diversity varies by distance or travel-time from the core of an urban region. Historically, literature on suburbanization in the social sciences assume a more heterogeneous urban core relative to segregated and homogeneous suburban neighborhoods. The diversity gradient is a visual heuristic used to evaluate the validity of this demographic model.

The entropy index for a given geographic unit is calculated as follows:

$$
E = formula()
$$

This statistic is implemented in the `entropy()` function in the segregation package. As the `entropy()` function calculates this statistic for a specific unit at a time, we can iterate over Census tracts with `map_dbl()` from the purrr package and compute the entropy index for each tract. The argument `base = 4` is set by convention to the number of groups in the calculation; this sets the maximum value of the statistic to 1, which represents perfect evenness between the four groups in the area. Once computed, the indices are joined to a dataset of Census tracts from California; `inner_join()` is used to retain only those tracts in the Los Angeles urbanized area.

```{r}
la_entropy <- la_data %>%
  split(~GEOID) %>%
  map_dbl(~{
    entropy(
      data = .x,
      group = "variable",
      weight = "estimate",
      base = 4
    )
  }) %>%
  as_tibble(rownames = "GEOID") %>%
  rename(entropy = value)

la_entropy_geo <- tracts("CA", cb = TRUE, year = 2019) %>%
  inner_join(la_entropy, by = "GEOID")

```

Visualization of the diversity gradient then requires a relative measurement of how far each Census tract is from the urban core. The travel-time methods available in the mapboxapi package introduced in Chapter \@ref(spatial-analysis-with-us-census-data) are again used here to calculate driving distance to Los Angeles City Hall for all Census tracts in the Los Angeles urbanized area.

```{r, eval = FALSE}
library(mapboxapi)

la_city_hall <- mb_geocode("City Hall, Los Angeles CA")

minutes_to_downtown <- mb_matrix(la_entropy_geo, la_city_hall)
```

Once computed, the travel times are stored in a vector `minutes_to_downtown`, then assigned to a new column `minutes` in the entropy data frame. The tract diversity index is visualized using ggplot2 relative to its travel time to downtown Los Angeles, with a LOESS smoother superimposed over the scatterplot to represent the diversity gradient.

```{r}
la_entropy_geo$minutes <- as.numeric(minutes_to_downtown)

ggplot(la_entropy_geo, aes(x = minutes_to_downtown, y = entropy)) + 
  geom_point(alpha = 0.5) + 
  geom_smooth(method = "loess") + 
  theme_minimal() + 
  scale_x_continuous(limits = c(0, 80)) + 
  labs(title = "Diversity gradient, Los Angeles urbanized area",
       x = "Travel-time to downtown Los Angeles in minutes, Census tracts",
       y = "Entropy index")

```

The diversity gradient shows...

## Regression modeling with US Census data

*Regression modeling* is widely used in industry and the social sciences to understand social processes. In the social sciences, the goal of regression modeling is commonly to understand the relationships between a variable under study, termed an *outcome variable*, and one or more *predictors* that are believed to have some influence on the outcome variable. In very simple terms, a regression model can be written mathematically as follows:

$$
Y = X\beta + \epsilon
$$

where $Y$ represents the outcome variable; $X$ is a matrix of predictors hypothesized to have some influence on the outcome variable; $\beta$ is a vector of *coefficients* that represent the modeled relationships between $X$ and $Y$, controlling for the other predictors in the model; and $\epsilon$ represents the *error terms* or residuals, the differences between the modeled values of $Y$ and the actual values. The relationships between $X$ and $Y$ will be modeled using a method appropriate for the structure of the data and selected by the analyst.

A complete treatment of regression modeling is beyond the scope of this book; recommended resources include X, Y, and Z. The purpose of this section is to illustrate an example workflow using regression modeling to analyze data from the American Community Survey. The section will start with a simple linear model and extend its discussion from there. In doing so, some problems with the application of the linear model to aggregated Census data will be discussed. First, demographic statistics are often highly correlated with one another, meaning that Census data-based models risk *multicollinearity* where predictors are not independent. Second, spatial demographic data commonly exhibit spatial autocorrelation, which may lead to a violation of the assumption of independent and identically distributed error terms ($i.i.d$) in the linear model. Suggested approaches for addressing these problems discussed in this section include dimension reduction and spatial regression.

### Data setup and exploratory data analysis

The topic of study in this illustrative applied workflow will be median home value by Census tract in the Dallas-Fort Worth metropolitan area. To get started, we'll define several counties in north Texas that we'll use to represent the DFW region, and acquire

```{r}
library(tidycensus)
library(tidyverse)
library(crsuggest)

dfw_counties <- c("Collin County", "Dallas", "Denton", "Ellis", "Hunt", 
                  "Kaufman", "Rockwall", "Johnson", "Parker", "Tarrant", "Wise")

median_value <- get_acs(
  geography = "tract",
  variables = "B25077_001",
  state = "TX",
  county = dfw_counties,
  geometry = TRUE
)

best_crs <- suggest_top_crs(median_value, units = "m")

ggplot(median_value, aes(fill = estimate)) + 
  geom_sf(color = NA) + 
  coord_sf(crs = best_crs) + 
  scale_fill_viridis_c(labels = scales::dollar) + 
  theme_void() + 
  labs(fill = "Median home value ")
```

-   Fitting a simple regression model

```{r}
library(sf)
library(units)

variables_to_get <- c(
  median_value = "B25077_001",
  median_rooms = "B25018_001",
  median_income = "DP03_0062",
  total_population = "B01003_001",
  median_age = "B01002_001",
  pct_college = "DP02_0068P",
  pct_foreign_born = "DP02_0094P",
  pct_white = "DP05_0077P",
  median_year_built = "B25037_001",
  percent_ooh = "DP04_0046P"
)

dfw_data <- get_acs(
  geography = "tract",
  variables = variables_to_get,
  state = "TX",
  county = dfw_counties,
  geometry = TRUE,
  output = "wide"
) %>%
  st_transform(best_crs) %>%
  mutate(pop_density = as.numeric(set_units(total_populationE / st_area(.), "1/km2")),
         median_structure_age = 2017 - median_year_builtE)

glimpse(dfw_data)
```

```{r simple-model}
dfw_data_for_model <- dfw_data %>%
  select(!ends_with("M")) %>%
  na.omit()

formula <- "log(median_valueE) ~ median_roomsE + median_incomeE + pct_collegeE + pct_foreign_bornE + pct_whiteE + median_ageE + median_structure_age + percent_oohE + pop_density + total_populationE"

model1 <- lm(formula = formula, data = dfw_data_for_model)

summary(model1)
```

```{r evaluate_model}
library(corrr)

dfw_estimates <- dfw_data_for_model %>%
  select(-GEOID, -NAME, -median_valueE, -median_year_builtE) %>%
  st_drop_geometry()

correlations <- correlate(dfw_estimates, method = "pearson")
```

```{r}
correlations %>% network_plot()
```

```{r check-vif}
car::vif(model1)
```

```{r components}

pca <- prcomp(formula = ~., data = dfw_estimates, scale. = TRUE, na.action = na.exclude)

summary(pca)
```

Perhaps plot the loadings then make a map of one of the components?

```{r}
pca$rotation
```

```{r pca_model}
components <- predict(pca, dfw_estimates)

dfw_pca <- dfw_data_for_model %>%
  select(GEOID, median_valueE) %>%
  cbind(components) %>%
  na.omit()

ggplot(dfw_pca, aes(fill = PC1)) +
  geom_sf(color = NA) +
  theme_void() +
  scale_fill_viridis_c()
```

```{r new-model}
pca_formula <- paste0("log(median_valueE) ~ ", paste0('PC', 1:8, collapse = ' + '))

pca_model <- lm(formula = pca_formula, data = dfw_pca)

summary(pca_model)

```

```{r residuals}

dfw_data_for_model$residuals <- residuals(model1)

ggplot(dfw_data_for_model, aes(fill = residuals)) +
  geom_sf(color = NA) +
  theme_void() +
  scale_fill_viridis_c()

```

Check for spatial autocorrelation in residuals:

```{r check-autocorrelation}
library(spdep)

nbrs <- dfw_data_for_model %>%
  poly2nb()

wts <- nb2listw(nbrs, style = "W")

moran.test(dfw_data_for_model$residuals, wts, na.action = na.exclude)
```

## Spatial regression

-   Fitting a spatial regression model
-   Spatial lag vs. spatial error models
-   Intepreting results

Show Lagrange multiplier test first

```{r}
lm.LMtests(model1, wts, test = "all")
```

```{r}
library(spatialreg)

lag_model <- lagsarlm(formula = formula, data = dfw_data_for_model, listw = wts)

summary(lag_model, Nagelkerke = TRUE)
```

Error model:

```{r}
error_model <- errorsarlm(formula = formula, data = dfw_data_for_model, listw = wts)

summary(error_model, Nagelkerke = TRUE)
```

Spatial Durbin model:

```{r}
durbin_model <- lagsarlm(formula = formula, data = dfw_data_for_model, listw = wts,
                         Durbin = TRUE)

summary(durbin_model, Nagelkerke = TRUE)
```

## Geographically weighted regression

-   Basic principles
-   Fitting model/mapping results
-   Dealing with potential issues (e.g. local multicollinearity)

```{r}
library(GWmodel)

dfw_data_sp <- dfw_data_for_model %>%
  as_Spatial()

bw <- bw.gwr(formula = formula, data = dfw_data_sp, kernel = "gaussian")
```

Fit the model:

```{r}
gw_model <- gwr.basic(formula = formula, data = dfw_data_sp, bw = bw,
                      kernel = "gaussian")

gw_model
```

Plot the results:

```{r}
gw_model_results <- gw_model$SDF %>%
  st_as_sf() 


ggplot(gw_model_results, aes(fill = median_roomsE)) + 
  geom_sf(color = NA) + 
  scale_fill_viridis_c() + 
  theme_void()

```

```{r}
bw2 <- bw.gwr(formula = formula, data = dfw_data_sp, kernel = "gaussian",
              adaptive = TRUE)
```

Adaptive bandwidth model results:

```{r, error = TRUE}
gw_model2 <- gwr.basic(formula = formula, data = dfw_data_sp, bw = bw2,
                      kernel = "gaussian", adaptive = TRUE)

gw_model2
```

```{r}
gw_model_results2 <- gw_model2$SDF %>%
  st_as_sf() 


ggplot(gw_model_results2, aes(fill = median_roomsE)) + 
  geom_sf(color = NA) + 
  scale_fill_viridis_c() + 
  theme_void()
```

## Geodemographic classification

```{r}
set.seed(1983)

dfw_kmeans <- dfw_pca %>%
  st_drop_geometry() %>%
  na.omit() %>%
  select(PC1:PC8) %>%
  kmeans(centers = 8)

table(dfw_kmeans$cluster)
```

Map the clusters:

```{r}
dfw_clusters <- dfw_pca %>%
  na.omit() %>%
  mutate(cluster = as.character(dfw_kmeans$cluster))

ggplot(dfw_clusters, aes(fill = cluster)) + 
  geom_sf(size = 0.1) + 
  scale_fill_brewer(palette = "Set1") + 
  theme_void()
```

## Spatial clustering & regionalization

-   Concept: clustering with spatial constraints
-   SKATER and minimum spanning trees
-   Potential applications
-   Alternative approaches

```{r}
input_vars <- dfw_pca %>%
  select(PC1:PC8) %>%
  st_drop_geometry() %>%
  as.data.frame() %>%
  na.omit()

skater_nbrs <- poly2nb(na.omit(dfw_pca), queen = TRUE)
costs <- nbcosts(skater_nbrs, input_vars)
skater_weights <- nb2listw(skater_nbrs, costs, style = "B")

mst <- mstree(skater_weights)

regions <- skater(mst[,1:2], input_vars, ncuts = 7,
                  crit = 10)
```

Plot the regions:

```{r}
dfw_clusters$region <- as.character(regions$group)

ggplot(dfw_clusters, aes(fill = region)) + 
  geom_sf(size = 0.1) + 
  scale_fill_brewer(palette = "Set1") + 
  theme_void()
```
